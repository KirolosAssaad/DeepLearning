<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Conditional NFT Generation using GANs</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark static-top">
    <div class="container">
      <a class="navbar-brand" href="../home.html">Practical Machine Deep Learning</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../home.html">Home
              <span class="sr-only">(current)</span>
            </a>
          </li>

        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h1 class="mt-5">Conditional NFT Generation using GANs</h1>
        <ul class="list-unstyled">
          <li>Ahmed Ashraf</li>
          <li>Akram Aziz</li>
        </ul>
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Problem Statement</h2>
		<p>
            Fully computer generated artwork has been an area of increasing interest in the NFT space. Various collections have been created that feature artwork exclusively generated by GANs, such as GAN Apes [13],  and GAN Nature [12]. Such collections have not been met with widespread success due to the heterogeneity of the resulting artwork, as well as the lack of any discernible properties or features. These shortcomings have impaired their ability to be particularly useful as “identifiers” as discussed above. 
            </br> </br> In this project, we aimed to create a system that could automate and facilitate the creation of generative art pieces that could supplement the limited set of artwork released within a said collection. The final artwork should match the art style and aesthetic, and be largely indistinguishable,  from the larger collection. It should also reflect the features corresponding to a set of user-defined input “properties”.
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Dataset</h2>

        <p>
            We have chosen to use the Bored Apes Yacht Club dataset  (link: https://www.kaggle.com/stanleyjzheng/bored-apes-yacht-club), due in part to its high usability value as well as the dimensions of all the files in the dataset.  sThe 1.46GB dataset contains 10,000 63x631 images that feature a homegenous art style. 
          </br></br>
            For preprocessing, we needed to resize all the images to 256x256 in order to comply with the StyleGAN2 architectural requirements. We also needed to reduce the number of channels from 4 to 3, by converting the images from RGBA to RGB. 
    </p>
    
    <br /> <!-- Empty Line before the image -->
    <div class="img-container" align="center">
        <!-- Block parent element -->
        <img src="resources/images/dataset-examples.jpeg" class="img-fluid text-center">
    </div>
    <br /> <!-- Empty Line after the image -->



    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Input/Output Examples</h2>
          <br /> <!-- Empty Line before the image -->
          <div class="img-container" align="center">
              <!-- Block parent element -->
              <img src="resources/images/input-output-examples.png" class="img-fluid text-center">
          </div>
          <br /> <!-- Empty Line after the image -->

      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">State of the art</h2>

        <p>
          The most recent contribution to literature that tackled the generation of NFT artwork through deep machine learning was NFTGan, which was based upon the style-transfer model StyleGAN2.
          </br></br>
          NFTGan was trained on a dataset of 2283 images that were resized to 512x512 for compatibility with StyleGAN2, using a Pytorch implementation of StyleGAN2 was then trained on the data set for 59 hours on a NVIDIA Tesla P100 GPU with 16 GB memory. 
        </br></br>
        <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/soa-examples.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
        After training, the authors reported an FID of 43.64 and a KID of 0.012
        <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/soa-results.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->

          
		</p>



    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Orignial Model from Literature</h2>

        <p>
          StyleGAN was conceived out of the need for more finely grained control over image generation applications using generative adversarial networks.  To that end, changes were introduced for a redesigned architecture for the generation module in order to improve image generation quality, which no significant upgrades being proposed for either the discriminator or the loss function.  
          </br></br>          
          Our main focus in this project is StyleGAN's replacement of the traditional input layer with a nonlinear multi-level perceptron feed forward network that maps the latent space into vector space W instead of directly feeding the vector z into the generator. 
        </p>

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/stylegan-input.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Proposed Updates</h2>

        <h4 class="mt-5">Model</h5>

          <h5 class="mt-5">Proposal 1</h5>
          <p>
            Our first experiment was to emulate the LoGAN model into StyleGAN 2, which  involved concatenating the unprocessed labels with the noised latent space vector. The concatenated vector is then passed into a multi-layer mapping network for disentanglment 
          </p>
          <br/> <!-- Empty Line before the image -->
            <div class="img-container" align="center"> <!-- Block parent element -->
                <img src="resources/images/proposal-1.png" class="img-fluid text-center">
            </div>
          <br/> <!-- Empty Line after the image -->


          <h5 class="mt-5">Proposal 2</h5>
          <p>
            Our second proposed model was to use the conditions system newly implemented into StyleGAN. This involved embedding the labels vector using a single dense layer, after which the processed labels  are then concatenated with the noised latent space vector. The concatenated vector is then passed into a multi-layer mapping network for disentanglment 
          </p>
          <br/> <!-- Empty Line before the image -->
            <div class="img-container" align="center"> <!-- Block parent element -->
                <img src="resources/images/proposal-2.png" class="img-fluid text-center">
            </div>
          <br/> <!-- Empty Line after the image -->

          <h5 class="mt-5">Proposal 3</h5>
          <p>
            Our third experiment was to pass the labels vector over a deep multi-layer embedding network. The processed labels  are then concatenated with the noised latent space vector. The concatenated vector is then passed into a multi-layer mapping network for disentanglment.
          </p>
          <br/> <!-- Empty Line before the image -->
            <div class="img-container" align="center"> <!-- Block parent element -->
                <img src="resources/images/proposal-3.png" class="img-fluid text-center">
            </div>
          <br/> <!-- Empty Line after the image -->


          <h5 class="mt-5">Proposal 4</h5>
          <p>
            Our final experiment was to skip the embedding altogether. The unprocessed labels are concatenated directly with the latent vector after its been through the disentanglement mapping network
          </p>
          <br/> <!-- Empty Line before the image -->
            <div class="img-container" align="center"> <!-- Block parent element -->
                <img src="resources/images/proposal-4.png" class="img-fluid text-center">
            </div>
          <br/> <!-- Empty Line after the image -->

          <h4 class="mt-5">Dataset</h5>
            <h5 class="mt-5">Dataset Labeling</h5>
            <p>As all potential datasets were unlabelled, we proposed and created a scrapper to scrape the labels for the dataset needed</p>


      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Results</h2>

        <h4 class="mt-5">Experiment 1</h2>    
      <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/ex-1-results.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
        </div>
      </div>

      <h4 class="mt-5">Experiment 2</h2>    
        <br/> <!-- Empty Line before the image -->
          <div class="img-container" align="center"> <!-- Block parent element -->
              <img src="resources/images/ex-2-results.png" class="img-fluid text-center">
          </div>
          <br/> <!-- Empty Line after the image -->
          </div>
        </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Technical report</h2>

	 	<ul>
		  <li>Programming framework: Python 3.7 venv supported by anaconda</li>
		  <li>Training hardware: Tesla P100 w/16GB of VRAM  - Provided by Google Colab Pro+ </li>
		  <li>Training time: An aggregate of 144 hours</li>
		  <li>Number of training cycles: 6,650 kimg (Chief Experiment)</li>
		  <li>Time per cycle: 120 seconds/kimg (on average)</li>
		</ul> 
      </div>
    </div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">Conclusion</h2>

	    <p>
        Our conclusions are two folds. We firmly believe that our results act as a good counterargument against qualitative metrics, including FID, for the evaluation of Generative Adverserial Networks. Our FID results for both experiments indicated no improve in the performance of the generator from inception till hour 96 of training. Regardless, our produced images showed close proximity to the optimal anticipated results. 
      </br> 
        We also believe that our results demonstrated that the LoGAN proposal for conditional generation is superior vis-a-vis to the latest StyleGAN proposal. 
		  </p>

	  </div>
	</div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">References</h2>
		<ol>
      <li> https://github.com/NVlabs/stylegan </li>
      <li> https://github.com/NVlabs/stylegan2-ada </li>
      <li> https://github.com/cedricoeldorf/ConditionalStyleGAN </li>
      <li> https://arxiv.org/pdf/2112.10577.pdf </li>
      <li> https://arxiv.org/pdf/1812.04948.pdf </li>
      <li> https://arxiv.org/pdf/2006.06676.pdf </li>
      <li> https://arxiv.org/pdf/1909.09974.pdf </li>
      <li> https://www.kaggle.com/vepnar/nft-art-dataset </li>
      <li> https://www.kaggle.com/stanleyjzheng/bored-apes-yacht-club </li>
      <li> https://www.kaggle.com/tunguz/cryptopunks </li>
      <li> https://www.kaggle.com/solpunks/solpunks-truth-project </li>
      <li> https://opensea.io/collection/gan-nature </li>
      <li> https://opensea.io/collection/gan-apes-nft </li>
      <li> https://docs.opensea.io/reference/api-overview </li> 
      <li> https://boredapeyachtclub.com/#/provenance </li>
      <li> https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/ </li>
      <li> https://www.forbes.com/sites/forbestechcouncil/2021/12/27/the-nft-metaverse-building-a-blockchain-world/ </li>
      <li> https://arxiv.org/pdf/1912.04958.pdf </li>
		</ol> 
	  </div>
	</div>

  </div>



  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.slim.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
